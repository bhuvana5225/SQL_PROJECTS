readme_content = """# E-commerce Datasets Analysis

## Overview
This project focuses on analyzing the operations of a leading e-commerce retailer (Target) in Brazil. The dataset, comprising eight CSV files, provides a comprehensive view of 100,000 orders placed between 2016 and 2018. The analysis aims to extract valuable insights and offer actionable recommendations for the company.

## Table of Contents
1. [Tech Stack](#tech-stack)
2. [Datasets](#datasets)
3. [Loading Datasets Into MySQL](#loading-datasets-into-mysql)
4. [Queries](#queries)


## Tech Stack
- **My SQL :** Used for querying the dataset and extracting relevant information. 
- **Python :** Utilized for additional data analysis and visualization.


## Datasets

The dataset is available in the google drive link given in datasets.txt file . It consists of eight CSV files.

1. **customers.csv**: Information about customers, including IDs, unique IDs, zip codes, cities, and states.
2. **sellers.csv**: Details about sellers, such as IDs, zip codes, cities, and states.
3. **order_items.csv**: Data on orders, including order and item IDs, product IDs, seller IDs, and pricing details.
4. **geolocation.csv**: Geographical information with zip codes, latitude, longitude, cities, and states.
5. **payments.csv**: Payment-related data, including order IDs, payment sequences, types, installments, and values.
6. **orders.csv**: Order details, including order IDs, customer IDs, statuses, timestamps, and delivery information.
7. **reviews.csv**: Customer reviews with review IDs, order IDs, scores, titles, comments, and timestamps.
8. **products.csv**:Product information, including IDs, categories, name lengths, description lengths, photos, weights, lengths, heights, and widths.

## Loading Datasets Into MySQL
```python
import pandas as pd
import mysql.connector
import os

# List of CSV files and their corresponding table names
csv_files = [
    ('customers.csv', 'customers'),
    ('geolocation.csv','geolocation'),
    ('orders.csv', 'orders'),
    ('sellers.csv', 'sellers'),
    ('products.csv', 'products'),
    ('order_items.csv', 'order_items'),
    ('order_reviews.csv', 'order_reviews'),
    ('payments.csv', 'payments')  # Added payments.csv for specific handling
]

# Connect to the MySQL database
conn = mysql.connector.connect(
    host='your_host',
    user='your_username',
    password='your_password',
    database='your_database'
)
cursor = conn.cursor()

# Folder containing the CSV files
folder_path = 'path_to_your_folder'

def get_sql_type(dtype):
    if pd.api.types.is_integer_dtype(dtype):
        return 'INT'
    elif pd.api.types.is_float_dtype(dtype):
        return 'FLOAT'
    elif pd.api.types.is_bool_dtype(dtype):
        return 'BOOLEAN'
    elif pd.api.types.is_datetime64_any_dtype(dtype):
        return 'DATETIME'
    else:
        return 'TEXT'

for csv_file, table_name in csv_files:
    file_path = os.path.join(folder_path, csv_file)
    
    # Read the CSV file into a pandas DataFrame
    df = pd.read_csv(file_path)
    
    # Replace NaN with None to handle SQL NULL
    df = df.where(pd.notnull(df), None)
    
    # Debugging: Check for NaN values
    print(f"Processing {csv_file}")
    print(f"NaN values before replacement:\n{df.isnull().sum()}\n")

    # Clean column names
    df.columns = [col.replace(' ', '_').replace('-', '_').replace('.', '_') for col in df.columns]

    # Generate the CREATE TABLE statement with appropriate data types
    columns = ', '.join([f'`{col}` {get_sql_type(df[col].dtype)}' for col in df.columns])
    create_table_query = f'CREATE TABLE IF NOT EXISTS `{table_name}` ({columns})'
    cursor.execute(create_table_query)

    # Insert DataFrame data into the MySQL table
    for _, row in df.iterrows():
        # Convert row to tuple and handle NaN/None explicitly
        values = tuple(None if pd.isna(x) else x for x in row)
        sql = f"INSERT INTO `{table_name}` ({', '.join(['`' + col + '`' for col in df.columns])}) VALUES ({', '.join(['%s'] * len(row))})"
        cursor.execute(sql, values)

    # Commit the transaction for the current CSV file
    conn.commit()

# Close the connection
conn.close()
```

## Queries
***Basic Queries***
1. List all unique cities where customers are located.
2. Count the number of orders placed in 2017.
3. Find the total sales per category.
4. Calculate the percentage of orders that were paid in installments.
5. Count the number of customers from each state.

***Intermediate Queries***
1. Calculate the number of orders per month in 2018.
2. Find the average number of products per order, grouped by customer city.
3. Calculate the percentage of total revenue contributed by each product category.
4. Identify the correlation between product price and the number of times a product has been purchased.
5. Calculate the total revenue generated by each seller, and rank them by revenue.

***Advanced Queries***
1. Calculate the moving average of order values for each customer over their order history.
2. Calculate the cumulative sales per month for each year.
3. Calculate the year-over-year growth rate of total sales.
4. Calculate the retention rate of customers, defined as the percentage of customers who make another purchase within 6 months of their first purchase.
5. Identify the top 3 customers who spent the most money in each year.
